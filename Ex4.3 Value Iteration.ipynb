{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.3 Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the algorithm for Value Iteration in the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Value Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "passed test: values of v elements\n",
      "passed test: pi is list of length=15\n",
      "passed test: values of pi elements\n",
      "PASSED: Value Iteration passcode = 9990-000\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "import numpy as np\n",
    "def value_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Value Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]                # init all state value estimates to 0\n",
    "    pi = state_count*[0]\n",
    "    \n",
    "    # (this section of code can be removed when actual implementation is added)\n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0][0]\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(state_count):\n",
    "            prev_v = V[s]\n",
    "            expected_returns = list(sum(t[2] * (t[1] + gamma * V[t[0]]) for t in get_transitions(s, a)) for a in get_available_actions(s))\n",
    "            V[s] = max(expected_returns)\n",
    "            delta = max(delta, abs(prev_v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        expected_returns = list(sum(t[2] * (t[1] + gamma * V[t[0]]) for t in get_transitions(s, a)) for a in avail_actions)\n",
    "        pi[s] = avail_actions[np.argmax(expected_returns)]\n",
    "    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n",
    "    return (V, pi)        # return both the final value function and the final policy\n",
    "\n",
    "tester.value_iteration_test(value_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
