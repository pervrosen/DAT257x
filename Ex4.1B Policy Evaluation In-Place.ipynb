{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.1B Policy Evaluation using in-place method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the algorithm for Iterative Policy Evaluation using the in-place approach in the below code cell.  In the in-place approach, one array holds the values being estimated for each state and the same array is used for estimates of states needed by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (in-place) passcode = 9991-562\n"
     ]
    }
   ],
   "source": [
    "import tester       # required for testing and grading your code\n",
    "import numpy as np\n",
    "\n",
    "def policy_eval_in_place(state_count, gamma, theta, get_policy_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified policy for the specified MDP:\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    'get_policy_actions' is the stochastic policy function - it takes a state parameter and returns list of tuples, \n",
    "        where each tuple is of the form: (action, probability).  It represents the policy being evaluated.\n",
    "    'get_transitions' is the state/reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]\n",
    "    # insert code here to evaluate the policy using the in-place approach \n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(state_count):\n",
    "            prev_v = V[s]\n",
    "            actions = get_policy_actions(s)\n",
    "            V[s] = sum(a[1] * sum(t[2] * (t[1] + gamma * V[t[0]]) for t in get_transitions(s, a[0])) for a in actions)\n",
    "            delta = max(delta, np.absolute(prev_v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "tester.policy_eval_in_place_test(policy_eval_in_place)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
